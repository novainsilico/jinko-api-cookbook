{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data table transformation cookbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import webbrowser\n",
    "from openai import OpenAI\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jinko specifics imports & initialization\n",
    "# Please fold this section and do not change\n",
    "import jinko_helpers as jinko\n",
    "\n",
    "# Connect to Jinko (see README.md for more options)\n",
    "jinko.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cookbooks specific variables\n",
    "\n",
    "# jinko folder id you want your data to be uploaded in\n",
    "FOLDER_ID = \"84b38c2d-9720-4305-9161-fe4e5aa05817\"\n",
    "# data table to transform\n",
    "SOURCE_FILE_NAME = \"invalid_data\"\n",
    "# OPTIONAL: Post the data table on jinko and get it afterwards\n",
    "POST_DATA_TABLE = True\n",
    "\n",
    "resources_dir = os.path.normpath(\n",
    "    \"./resources/data_table_transformation\")\n",
    "source_file = os.path.join(resources_dir, SOURCE_FILE_NAME + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils functions\n",
    "\n",
    "\n",
    "def run_code_on_dataframe(df, transform_data):\n",
    "    \"\"\"\n",
    "    Executes a transformation function (in that case, it's provided by the LLM) on a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: A pandas DataFrame on which the transformation is to be applied.\n",
    "        transform_data: A function that takes a DataFrame as input and returns\n",
    "                        a transformed DataFrame or any result.\n",
    "\n",
    "    Returns:\n",
    "        The result of the transformation function if successful, otherwise an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = transform_data(df)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        error_message = f\"An error occurred: {e}\"\n",
    "        print(error_message)\n",
    "        return error_message\n",
    "\n",
    "\n",
    "def post_data_table(source_file):\n",
    "    \"\"\"\n",
    "    Post the data table to jinko.\n",
    "\n",
    "    Args:\n",
    "        source_file (str): Path to the CSV file containing the data table.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (data_table_core_item_id, data_table_snapshot_id)\n",
    "    \"\"\"\n",
    "    encoded_data_table = jinko.data_table_to_sqlite(source_file)\n",
    "    # https://doc.jinko.ai/api/#/paths/core-v2-data_table_manager-data_table/post\n",
    "    response = jinko.make_request(\n",
    "        path=\"/core/v2/data_table_manager/data_table\",\n",
    "        method=\"POST\",\n",
    "        json={\n",
    "            \"mappings\": [],\n",
    "            \"rawData\": encoded_data_table,\n",
    "        },\n",
    "        options={\n",
    "            \"name\": SOURCE_FILE_NAME,\n",
    "            \"folder_id\": FOLDER_ID,\n",
    "        },\n",
    "    )\n",
    "    response_json = response.json()\n",
    "    data_table_core_item_id = response_json.get(\"coreItemId\")\n",
    "    data_table_snapshot_id = response_json.get(\"snapshotId\")\n",
    "    data_table_url = jinko.get_project_item_url_by_core_item_id(\n",
    "        data_table_core_item_id)\n",
    "    webbrowser.open(data_table_url)\n",
    "    return (data_table_core_item_id, data_table_snapshot_id)\n",
    "\n",
    "\n",
    "def load_data_table(data_table_core_item_id, data_table_snapshot_id):\n",
    "    \"\"\"\n",
    "    Load a data table snapshot from Jinko and return it as a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data_table_core_item_id (str): The core item ID of the data table.\n",
    "        data_table_snapshot_id (str): The snapshot ID of the data table.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A pandas DataFrame containing the data table, with the \"rowId\"\n",
    "        column removed.\n",
    "    \"\"\"\n",
    "\n",
    "    r = jinko.make_request(\n",
    "        f\"/core/v2/data_table_manager/data_table/{data_table_core_item_id}/snapshots/{data_table_snapshot_id}/export\",\n",
    "        method=\"POST\",\n",
    "        options={\"output_format\": \"text/csv\"},\n",
    "    )\n",
    "    csv_data = r.text\n",
    "    df = pd.read_csv(io.StringIO(csv_data)).drop(columns=[\"rowId\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_dropped_columns(df, scoring_grouped):\n",
    "    \"\"\"\n",
    "    Generate a DataFrame that contains info about which columns from the original DataFrame were dropped\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The original DataFrame\n",
    "    scoring_grouped : pd.DataFrame\n",
    "        The DataFrame grouped by the scoring group\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dropped_columns_df : pd.DataFrame\n",
    "        A DataFrame with the following columns:\n",
    "        raw_columns : str\n",
    "            The name of the column in the original DataFrame\n",
    "        dropped : bool\n",
    "            Whether the column was dropped or not\n",
    "        type : str\n",
    "            The type of the column in the original DataFrame\n",
    "    \"\"\"\n",
    "    raw_columns = set(df.columns)\n",
    "    scoring_grouped_obs_ids = set(scoring_grouped[\"obsId\"])\n",
    "    scoring_grouped_columns = set(scoring_grouped.columns)\n",
    "    all_posible_values = scoring_grouped_obs_ids.union(scoring_grouped_columns)\n",
    "    dropped_column_data = []\n",
    "    for col in raw_columns:\n",
    "        dtype_str = str(df[col].dtype)\n",
    "        if col in all_posible_values:\n",
    "            dropped_column_data.append(\n",
    "                {\"raw_columns\": col, \"dropped\": False, \"type\": dtype_str}\n",
    "            )\n",
    "        else:\n",
    "            dropped_column_data.append(\n",
    "                {\"raw_columns\": col, \"dropped\": True, \"type\": dtype_str}\n",
    "            )\n",
    "    dropped_columns_df = pd.DataFrame(dropped_column_data)\n",
    "    return dropped_columns_df\n",
    "\n",
    "\n",
    "def generate_dropped_observables(observable_names, scoring_grouped):\n",
    "    \"\"\"\n",
    "    Generate a DataFrame indicating which observables were dropped.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    observable_names : pd.DataFrame\n",
    "        A DataFrame containing observable IDs and their types.\n",
    "    scoring_grouped : pd.DataFrame\n",
    "        A DataFrame containing grouped scoring data with observable IDs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dropped_obs_id_df : pd.DataFrame\n",
    "        A DataFrame with columns:\n",
    "        obsId : str\n",
    "            The ID of the observable.\n",
    "        dropped : bool\n",
    "            Whether the observable was dropped (True) or not (False).\n",
    "        type : str\n",
    "            The data type of the observable.\n",
    "    \"\"\"\n",
    "\n",
    "    observable_ids = set(observable_names[\"obsId\"])\n",
    "    scoring_grouped_obs_ids = set(scoring_grouped[\"obsId\"])\n",
    "    dropped_obs_id_data = []\n",
    "    for obs_id in observable_ids:\n",
    "        dtype_str = str(\n",
    "            observable_names[observable_names[\"obsId\"]\n",
    "                             == obs_id][\"type\"].values[0]\n",
    "        )\n",
    "        if obs_id in scoring_grouped_obs_ids:\n",
    "            dropped_obs_id_data.append(\n",
    "                {\"obsId\": obs_id, \"dropped\": False, \"type\": dtype_str}\n",
    "            )\n",
    "        else:\n",
    "            dropped_obs_id_data.append(\n",
    "                {\"obsId\": obs_id, \"dropped\": True, \"type\": dtype_str}\n",
    "            )\n",
    "    dropped_obs_id_df = pd.DataFrame(dropped_obs_id_data)\n",
    "    return dropped_obs_id_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Data table loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if POST_DATA_TABLE:\n",
    "    data_table_core_item_id, data_table_snapshot_id = post_data_table(\n",
    "        source_file)\n",
    "    df = load_data_table(data_table_core_item_id, data_table_snapshot_id)\n",
    "else:\n",
    "    df = pd.read_csv(source_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : prompt generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = f\"\"\"\n",
    "You are a Python expert.\n",
    "\n",
    "Please help to create the function `transform_data` to process and structure the raw data. Do not provide a script, just the function `transform_data`. Your response should ONLY be based on the given context and follow the response guidelines and format instructions.\n",
    "\n",
    "# Response Guidelines\n",
    "\n",
    "1. If the provided context is sufficient, generate the `transform_data` function to follow the required steps\n",
    "2. If the provided context is insufficient, do not generate the Python function. Instead, briefly explain why it cannot be generated and request the missing information.\n",
    "3. Use the most relevant information from the raw data table, summary data and meta data.\n",
    "4. Format the Python query before responding.\n",
    "5. Ensure the Python function is syntactically and logically correct before finalizing.\n",
    "6. Follow the exact order of table creation as described below.\n",
    "7. Use real column names from the raw data table. If column names are missing, request clarification instead of assuming placeholder names.\n",
    "8. Pay close attention to the sections starting with “IMPORTANT:”\n",
    "9. Do not include data in the response, I only want the function transform_data\n",
    "\n",
    "# Response Format\n",
    "\n",
    "## If Python Generation is Successful\n",
    "\n",
    "  - Formatted Python code\n",
    "  - the output of the python function The `transform_data` function must return the dictionary outlined below:\n",
    "\\\\`\\\\`\\\\`python\n",
    "{{ \"observable_names\":<observable_names_df>\n",
    ", \"transposed_table\":<transposed_table_df>\n",
    ", \"scoring_grouped\":<scoring_grouped_df>}}\n",
    "\\\\`\\\\`\\\\`\n",
    "\n",
    "## If Python Generation is Not Possible\n",
    "  - A brief, human-readable explanation of why the query cannot be generated.\n",
    "  - A request for any missing information from the user.\n",
    "\n",
    "# Format instructions\n",
    "\n",
    "IMPORTANT: When generating the code, follow the exact sequence of instructions outlined below: \n",
    "\n",
    "## Observable table\n",
    "List all observables (i.e., patient characteristics) from the raw data table. \n",
    "The `observable_names` table schema is:\n",
    "\n",
    "observable_names (\n",
    "  obsId TEXT, -- identify the list of observables from a single column (the column contains observable names) or from several columns (one per observable). \n",
    "   type TEXT -- data type by observable\n",
    ")\n",
    "\n",
    "IMPORTANT: Follow these recommendations when building the table `observable_names`:\n",
    "\n",
    "1. ONLY for the construction of the table `observable_names`: Include both numeric and non-numeric observables INCLUDING variables such as: gender (F/M), tumor grade (A, B, C, etc.), phenotype (blue, red, etc.), genotype (wild, mutant, etc.), serostatus (positive, negative, etc.), and response to treatment (PR, partial response, etc.).\n",
    "2. Do not exclude categorical data at this stage.\n",
    "\n",
    "\n",
    "## Transposed table\n",
    "This table transforms the raw data into a structured format by organizing observables.\n",
    "The `transposed_table` table schema is:\n",
    "\n",
    "transposed_table (\n",
    "    patientId TEXT,  -- OPTIONAL: Include if patient identifiers exist in the raw data.\n",
    "   obsId TEXT, -- Extracted from the table `observable_names`. \n",
    "   time TEXT, -- Represents time durations. It is a concatenation of the TIME UNIT in TEXT and the TIME VALUE in NUMERIC. ISO-8601 duration format (e.g., \"P3Y6M4DT12H30M5S\"). \n",
    "   value NUMERIC, -- can be REAL or INTEGER. Observable values can be drawn from multiple data columns or from a single column if observables are listed separately. \n",
    "   arm TEXT, -- OPTIONAL: Usually a string like placebo, treated, or the name of a treatment or groups (group 1, group 2, group 3) or numbers (1, 2, 3)\n",
    "   narrowRangeLowBound NUMERIC, -- Can be REAL, INTEGER or NULL. ONLY if available in the raw tabular data otherwise NULL\n",
    "   narrowRangeHighBound NUMERIC -- Can be REAL, INTEGER or NULL. ONLY if available in the raw tabular data otherwise NULL\n",
    ")\n",
    "\n",
    "IMPORTANT: You must take into account when creating your Python request the following elements:\n",
    "1. For the TIME VALUES, if the TIME UNIT (in the same column or in another column indicating the time units) or the TIME VALUE is missing THEN NULL. IF not NULL then USE capital letters P, Y, M, W, D, T, H, M, and S as designators for EACH of the duration elements. For example, \"P3Y6M4DT12H30M5S\" represents a duration of \"three years, six months, four days, twelve hours, thirty minutes, and five seconds\". See: https://en.wikipedia.org/wiki/ISO_8601#Durations\n",
    "2. EXCLUDE NON-NUMERIC OBSERVABLES, including categorical data coded as numeric but with only a few distinct values (e.g., binary (0,1) or tumor stages (1,2,3,4), which are not considered observables).\n",
    "3. In the column “VALUE”, check for a mix of formats, and any cell value (each cell) that is not exclusively numeric or contains a missing data representation (e.g., NULL, NA, n.a., empty cell, and other language-specific codes) must be considered a missing value and converted to NULL.\n",
    "4. DO NOT DROP ROWS with missing values in this table. So, missing values must be retained in `transposed_table` but will be ignored during aggregation in `scoring_grouped` table.\n",
    "5. If a column contains only None or null values, you can drop it\n",
    "\n",
    "## scoring_grouped table\n",
    "If the \"arm\" column exists and is not null, group the data by \"arm\", \"time\", and \"obsId\". Otherwise, group by \"time\" and \"obsId\".\n",
    "\n",
    "scoring_grouped (\n",
    "   arm TEXT -- OPTIONAL, \n",
    "   time TEXT, -- the format is ISO-8601\n",
    "   obsId TEXT,\n",
    "   value REAL, -- the reduction function is by default the mean of values. If the user specifies another reduction function use the specified one instead.\n",
    "   narrowRangeLowBound NUMERIC, -- Can be REAL, INTEGER or NULL. Default is the min of values. If the user specifies another reduction function, use the specified one instead. \n",
    "   narrowRangeHighBound NUMERIC --Can be REAL, INTEGER or NULL. Default max of values. If the user specifies another reduction function, use the specified one instead.\n",
    ")\n",
    "\n",
    "IMPORTANT: Follow these recommendations when building the 'scoring_grouped' table:\n",
    "1. Create a new table `scoring_grouped` based on the `transposed_table`.\n",
    "2.  This new table is a grouping of patients. The grouping SHOULD BE DONE FOR THE TRIPLET (“arm”, ”time”, “obsId”) unless otherwise specified by the user. If `arm` doesn't exist or if its value is null, the aggregation will be done on the pairs (“time”, “obsId”). Use the reduction function by default and if the user specifies another reduction function, use the specified one instead.\n",
    "3. For each aggregation, ignore rows with missing “time” values OR  missing observable values (NULL values).\n",
    "4. If the \"narrowRangeLowBound\" and \"narrowRangeHighBound\" columns exist in the `transposed_table`, use the existing ones instead of calculating new values.\n",
    "5. Order the values of the `scoring_grouped` table in ascending order by the triplet (“arm”, “time”, “obsId”): first by the “time” column if it exists, then the “arm” column if it exists, and finally by the “obsId” column.\n",
    "6. missing time values should be stored as NULL in `transposed_table` and only ignored in `scoring_grouped`. \n",
    "\n",
    "# Question\n",
    "\n",
    "Generate a series of Python queries to transform the raw data table into a structured set of tables. Follow the steps in the specified order:\n",
    "\n",
    "1. observable_names\n",
    "2. transposed_table\n",
    "3. scoring_grouped\n",
    "\n",
    "\n",
    "# Final Consistency Check\n",
    "Before finalizing, ensure:\n",
    "\n",
    "1. The function follows the order of operations:\n",
    "observable_names → transposed_table → scoring_grouped\n",
    "2. No syntax errors or logical inconsistencies are present.\n",
    "3. The output tables are consistent with each other.\n",
    "4. A message is included confirming successful Python generation.\n",
    "\n",
    "\n",
    "IMPORTANT: Ensure the generated Python function is free of syntax errors and logically consistent before finalizing the response!\n",
    "\n",
    "# Final Instruction to Assistant\n",
    "- If the Python function is successfully generated:\n",
    "  - return only the function transform_data, with no surrounding text or formatting and especially no ```python surronding\n",
    "  - do not include any messages indicating success.\n",
    "\n",
    "\n",
    "- If the Python function cannot be generated, provide a brief explanation and request missing details.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a summary of the DataFrame\n",
    "summary = df.describe()\n",
    "head = df.head()\n",
    "data_types = df.dtypes\n",
    "unique_values = df.nunique()\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Value counts for categorical columns\n",
    "value_counts = {}\n",
    "for column in df.select_dtypes(include=[\"object\"]).columns:\n",
    "    value_counts[column] = df[column].value_counts()\n",
    "\n",
    "user_message = f\"\"\"\n",
    "# Data Summary for Transformation\n",
    "\n",
    "## Data Types\n",
    "```\n",
    "{data_types}\n",
    "```\n",
    "\n",
    "## Unique Values\n",
    "```\n",
    "{unique_values}\n",
    "```\n",
    "\n",
    "## Missing Values\n",
    "```\n",
    "{missing_values}\n",
    "```\n",
    "\n",
    "## Value Counts\n",
    "```\n",
    "{value_counts}\n",
    "```\n",
    "\n",
    "## Statistical Summary\n",
    "```\n",
    "{summary}\n",
    "```\n",
    "\n",
    "## Example Data\n",
    "```\n",
    "{head}\n",
    "```\n",
    "\n",
    "# Task\n",
    "Write the function `transform_data` that transforms a given data table into the specified format.\n",
    "\"\"\"\n",
    "\n",
    "print(user_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: python function creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message},\n",
    "]\n",
    "\n",
    "openai_client = OpenAI()\n",
    "MODEL = \"o4-mini\"\n",
    "\n",
    "response = openai_client.responses.create(model=MODEL, input=chat_history)\n",
    "\n",
    "code = response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run the generated python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(code)\n",
    "code_result = run_code_on_dataframe(\n",
    "    df, transform_data\n",
    ")  # we assume transform_data has just been executed\n",
    "\n",
    "observable_names = code_result[\"observable_names\"]\n",
    "transposed_table = code_result[\"transposed_table\"]\n",
    "scoring_grouped = code_result[\"scoring_grouped\"]\n",
    "\n",
    "display(\"observable_names\", observable_names)\n",
    "display(\"transposed_table\", transposed_table)\n",
    "display(\"scoring_grouped\", scoring_grouped)\n",
    "display(\n",
    "    \"dropped_observables\",\n",
    "    generate_dropped_observables(observable_names, scoring_grouped),\n",
    ")\n",
    "display(\"dropped_columns\", generate_dropped_columns(df, scoring_grouped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Push the table \"scoring_grouped\" to jinko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_table = jinko.df_to_sqlite(scoring_grouped)\n",
    "\n",
    "\n",
    "if POST_DATA_TABLE:\n",
    "    response = jinko.make_request(\n",
    "        path=f\"/core/v2/data_table_manager/data_table/{data_table_core_item_id}\",\n",
    "        method=\"PUT\",\n",
    "        json={\n",
    "            \"mappings\": [],\n",
    "            \"rawData\": encoded_data_table,\n",
    "        },\n",
    "        options={\n",
    "            \"version_name\": \"transformed \" + SOURCE_FILE_NAME,\n",
    "        },\n",
    "    )\n",
    "\n",
    "else:\n",
    "\n",
    "    response = jinko.make_request(\n",
    "        path=\"/core/v2/data_table_manager/data_table\",\n",
    "        method=\"POST\",\n",
    "        json={\n",
    "            \"mappings\": [],\n",
    "            \"rawData\": encoded_data_table,\n",
    "        },\n",
    "        options={\n",
    "            \"name\": \"transformed \" + SOURCE_FILE_NAME,\n",
    "            \"folder_id\": FOLDER_ID,\n",
    "        },\n",
    "    )\n",
    "\n",
    "# Pretty print the response\n",
    "response_json = response.json()\n",
    "\n",
    "data_table_core_item_id = response_json.get(\"coreItemId\")\n",
    "data_table_snapshot_id = response_json.get(\"snapshotId\")\n",
    "\n",
    "data_table_url = jinko.get_project_item_url_by_core_item_id(\n",
    "    data_table_core_item_id)\n",
    "webbrowser.open(data_table_url)\n",
    "\n",
    "is_valid_for_calibration = jinko.make_request(\n",
    "    f\"/core/v2/data_table_manager/data_table/{data_table_core_item_id}/snapshots/{data_table_snapshot_id}\"\n",
    ").json()[\"validations\"][\"fitnessFunction\"]\n",
    "if is_valid_for_calibration:\n",
    "    print(\"\\033[92mThe data table is valid for calibration.\\033[0m\")\n",
    "\n",
    "else:\n",
    "    print(\"\\033[91mThe data table is not valid for calibration.\\033[0m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
