{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d54b1324-9c03-46f7-b920-5ff92c95123e",
   "metadata": {},
   "source": [
    "# Quantifying uncertainty in simulation results\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The goal of this cookbook is to generate an uncertainty quantification report from a given calibration.\n",
    "It takes a completed calibration as input, and outputs an exploratory trial, as well as a summary graph within the notebook.\n",
    "\n",
    "### Steps\n",
    "\n",
    "- Step 0: Select calibration of interest\n",
    "- Step 1: Load the calibration and get the last completed version\n",
    "- Step 2: Getting the maximum likelihood estimate (MLE)\n",
    "- Step 3: Generate A Vpop\n",
    "- Step 4: Create or update the exploratory Trial\n",
    "- Step 5: Run and monitor the trial\n",
    "- Step 6: Load and process the trial results\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Associated folder on jinko](https://jinko.ai/project/e0fbb5bb-8929-439a-bad6-9e12d19d9ae4?labels=e8479394-99ec-4297-83b7-e8b00511c185)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138dd704-b7d0-42f4-867c-ea4f80c1adb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jinko specifics imports & initialization\n",
    "# Please fold this section and do not edit it\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../lib\")\n",
    "import jinko_helpers as jinko\n",
    "\n",
    "# Connect to Jinko (see README.md for more options)\n",
    "jinko.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9944f60-72f7-40ae-b85a-af6ebbdcae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cookbook specifics imports\n",
    "import io\n",
    "import uuid\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import qmc\n",
    "import zipfile\n",
    "import textwrap\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, minmax_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ea9573",
   "metadata": {},
   "source": [
    "## Step 0: Select calibration of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f9fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calibration short id can be retrieved from the URL of your calibration in Jinko, pattern is `https://jinko.ai/<calibration_short_id>`\n",
    "\n",
    "WARNING: we cache the exploration trial ID such that subsequent runs of the notebook create new versions\n",
    "of the same Trial instead of new Trials each time.\n",
    "When CHANGING the calibration of interest, you will want to delete the cache by restarting the kernel (there should be a \"Restart\" or \"Restart kernel\" button somewhere)\n",
    "\"\"\"\n",
    "\n",
    "calibration_short_id = \"ca-pckt-0FOX\"\n",
    "\n",
    "# folder ID, pattern is `https://jinko.ai/project/<project_id>?labels=<folder_id>`\n",
    "# This folder is where the exploratory Vpop and Trial will be saved, it does not have to be the same folder as that of\n",
    "# the initial calibration\n",
    "folder_id = \"da41d151-af37-4c28-8e27-9f2a8ccf9895\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6829c0-29c3-4ac7-a0ec-68f5d542c48c",
   "metadata": {},
   "source": [
    "## Step 1: Pick the calibration version of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f24608-a47d-4339-b683-25e3a041f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a specific revision. By default we return the last version\n",
    "revision = None\n",
    "# Choose a specific label. By default we return the last version\n",
    "label = \"cookbook\"\n",
    "response = jinko.get_project_item(\n",
    "    sid=calibration_short_id, revision=revision, label=label\n",
    ")\n",
    "calibration_core_item_id, calibration_snapshot_id = (\n",
    "    response[\"coreId\"][\"id\"],\n",
    "    response[\"coreId\"][\"snapshotId\"],\n",
    ")\n",
    "\n",
    "# # Uncomment the following if you want to use the latest completed or stopped version\n",
    "# response = jinko.get_latest_calib_with_status(shortId=calibration_short_id, statuses=[\"completed\", \"stopped\"])\n",
    "# calibration_core_item_id, calibration_snapshot_id = response[\"coreItemId\"], response[\"snapshotId\"]\n",
    "\n",
    "print(\n",
    "    f\"Picked Calibration with coreItemId: {calibration_core_item_id}, snapshotId: {calibration_snapshot_id}\"\n",
    ")\n",
    "\n",
    "calibration_inputs = jinko.make_request(\n",
    "    f\"/core/v2/calibration_manager/calibration/{calibration_core_item_id}/snapshots/{calibration_snapshot_id}\",\n",
    ").json()\n",
    "parameters = [p[\"id\"] for p in calibration_inputs[\"parameters\"]]\n",
    "parameter_map = {p[\"id\"]: p for p in calibration_inputs[\"parameters\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a99d77a-e6aa-4f7f-97e7-287759b73249",
   "metadata": {},
   "source": [
    "## Step 2: Getting the maximum likelihood estimate (MLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8fa188-1157-41a5-b3a6-0437ba55d482",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = jinko.make_request(\n",
    "    f\"/core/v2/calibration_manager/calibration/{calibration_core_item_id}/snapshots/{calibration_snapshot_id}/results_summary\",\n",
    "    method=\"GET\",\n",
    ")\n",
    "arm_names = json.loads(response.content.decode(\"utf-8\"))[\"arms\"]\n",
    "\n",
    "response = jinko.make_request(\n",
    "    f\"/core/v2/result_manager/calibration/{calibration_core_item_id}/snapshots/{calibration_snapshot_id}/sorted_patients\",\n",
    "    method=\"POST\",\n",
    "    json={\n",
    "        \"sortBy\": \"optimizationWeightedScore\"\n",
    "    },\n",
    ")\n",
    "response_summary = json.loads(response.content)\n",
    "best_patient_index = response_summary[0]\n",
    "\n",
    "# Get the list of scalars that we want to download, they correspond to the calibration objectives\n",
    "objectives = jinko.make_request(\n",
    "    f\"/core/v2/calibration_manager/calibration/{calibration_core_item_id}/snapshots/{calibration_snapshot_id}/objectives_weights\",\n",
    "    method=\"GET\",\n",
    ").json()\n",
    "\n",
    "arm_names_with_cross_arms = [*arm_names, \"crossArms\"]\n",
    "response = jinko.make_request(\n",
    "    f\"/core/v2/result_manager/calibration/{calibration_core_item_id}/snapshots/{calibration_snapshot_id}/scalars/per_patient\",\n",
    "    method=\"POST\",\n",
    "    json={\n",
    "        \"arms\": arm_names_with_cross_arms,\n",
    "        # all scalars\n",
    "        \"scalars\": [],\n",
    "        \"iteration\": int(best_patient_index[\"iteration\"]),\n",
    "        \"patientNumber\": best_patient_index[\"patientNumber\"],\n",
    "    },\n",
    ")\n",
    "best_patient_raw = json.loads(response.content)\n",
    "cross_arm_values = list(\n",
    "    filter(\n",
    "        lambda x: x[\"scenarioArm\"] == \"crossArms\",\n",
    "        best_patient_raw[\"outputs\"],\n",
    "    )\n",
    ")\n",
    "if len(cross_arm_values) == 0:\n",
    "    raise ValueError(f\"Expected at least one cross-arm value\")\n",
    "best_patient_values = cross_arm_values[0][\"res\"]\n",
    "best_patient_parameters = {\n",
    "    p[\"id\"]: float(p[\"value\"])\n",
    "    for p in best_patient_values\n",
    "    if p[\"type\"][\"type\"] == \"BaselineInput\"\n",
    "}\n",
    "best_patient_scores = {\n",
    "    s[\"id\"]: s[\"value\"]\n",
    "    for results in best_patient_raw[\"outputs\"]\n",
    "    for s in results[\"res\"]\n",
    "    if (s[\"id\"] in objectives.keys() or s[\"id\"] == \"optimizationWeightedScore\")\n",
    "}\n",
    "mle = best_patient_scores[\"optimizationWeightedScore\"]\n",
    "print(\n",
    "    f\"Best patient corresponds to a maximum log-likelihood of {mle:.2g} (theoretical maximum is 1)\"\n",
    ")\n",
    "print(\"All scores of the best patient:\")\n",
    "display(best_patient_scores)\n",
    "best_patient_scores_trans = {\n",
    "    k: np.exp(best_patient_scores[k] - 1) for k in best_patient_scores\n",
    "}\n",
    "print(\"Best patient descriptors:\")\n",
    "display(best_patient_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32643282",
   "metadata": {},
   "source": [
    "## Step 3: Generate A Vpop\n",
    "\n",
    "Here we generate an exploratory Vpop without going through any Vpop Design.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8278d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_patient(x):\n",
    "    return {\n",
    "        \"patientIndex\": str(uuid.uuid4()),\n",
    "        \"patientCategoricalAttributes\": [],\n",
    "        \"patientAttributes\": [{\"id\": p, \"val\": x[j]} for j, p in enumerate(parameters)],\n",
    "    }\n",
    "\n",
    "\n",
    "# Defines the low and high bounds to explore for each parameter which is not in the \"custom_range\" map\n",
    "low_multiply = 1 / 2  # multiplies the MLE value to get the low bound\n",
    "high_multiply = 2  # multiplies the MLE value to get the high bound\n",
    "\n",
    "# Set custom ranges and logscale here. Simply add a new entry for each parameter that needs to be tweaked\n",
    "# Log scale is in base 10\n",
    "custom_range = {\n",
    "    \"kClearanceDrug\": {\"logscale\": True, \"min\": -3, \"max\": 0},\n",
    "    \"bloodFlowRate\": {\"logscale\": False, \"min\": 1, \"max\": 50},\n",
    "}\n",
    "\n",
    "\n",
    "def parameter_low_bound(param_name):\n",
    "    p = parameter_map[param_name]\n",
    "    if (\n",
    "        param_name in custom_range.keys()\n",
    "        and custom_range[param_name][\"min\"] is not None\n",
    "    ):\n",
    "        return custom_range[param_name][\"min\"]\n",
    "    elif p[\"minBound\"] is not None:\n",
    "        return max(\n",
    "            best_patient_parameters[f\"{param_name}.tmin\"] * low_multiply, p[\"minBound\"]\n",
    "        )\n",
    "    else:\n",
    "        return best_patient_parameters[f\"{param_name}.tmin\"] * low_multiply\n",
    "\n",
    "\n",
    "def parameter_high_bound(param_name):\n",
    "    p = parameter_map[param_name]\n",
    "    if (\n",
    "        param_name in custom_range.keys()\n",
    "        and custom_range[param_name][\"max\"] is not None\n",
    "    ):\n",
    "        return custom_range[param_name][\"max\"]\n",
    "    elif p[\"maxBound\"] is not None:\n",
    "        return min(\n",
    "            best_patient_parameters[f\"{param_name}.tmin\"] * high_multiply, p[\"maxBound\"]\n",
    "        )\n",
    "    else:\n",
    "        return best_patient_parameters[f\"{param_name}.tmin\"] * high_multiply\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "First we generate an exploratory Vpop using Sobol sequences to sample from the [-50%, +50%] hypercube\n",
    "around the MLE parameter values\n",
    "\"\"\"\n",
    "# The size of the exploratory Vpop is 2^m, e.g. vpop size is 1024 for m=10\n",
    "m = 10\n",
    "\n",
    "d = len(parameters)\n",
    "sampler = qmc.Sobol(d, scramble=False)\n",
    "samples = sampler.random_base2(m)\n",
    "samples = qmc.scale(\n",
    "    samples,\n",
    "    [parameter_low_bound(param_name) for param_name in parameters],\n",
    "    [parameter_high_bound(param_name) for param_name in parameters],\n",
    ")\n",
    "# Handle log scaled parameters\n",
    "for j, param_name in enumerate(parameters):\n",
    "    if (\n",
    "        param_name in custom_range.keys()\n",
    "        and custom_range[param_name][\"logscale\"] == True \n",
    "    ):\n",
    "        samples[:, j] = np.exp(np.log(10) * samples[:, j])\n",
    "exploratory_vpop = [to_patient(x) for x in samples]\n",
    "\n",
    "\"\"\"\n",
    "Then we generate a \"tensorized\" Vpop where, for each parameter p_i, we linearly sample on the [-50%, +50%]\n",
    "interval while keeping p_j,j/=i fixed to the MLE values\n",
    "\"\"\"\n",
    "# how many samples in the search interval\n",
    "n = 21\n",
    "\n",
    "patient_groups = {}\n",
    "tensor_vpop = []\n",
    "for param_name in parameters:\n",
    "    param_vals = np.linspace(\n",
    "        parameter_low_bound(param_name), parameter_high_bound(param_name), n\n",
    "    )\n",
    "    if (\n",
    "        param_name in custom_range.keys()\n",
    "        and custom_range[param_name][\"logscale\"] == True\n",
    "    ):\n",
    "        param_vals = np.exp(np.log(10) * param_vals)\n",
    "    all_vals = [\n",
    "        [\n",
    "            x if p == param_name else best_patient_parameters[f\"{p}.tmin\"]\n",
    "            for p in parameters\n",
    "        ]\n",
    "        for x in param_vals\n",
    "    ]\n",
    "    patients = [to_patient(x) for x in all_vals]\n",
    "    patient_groups[param_name] = set(patient[\"patientIndex\"] for patient in patients)\n",
    "    tensor_vpop += patients\n",
    "\n",
    "\"\"\"\n",
    "We assemble a map from patient_id to a 'label' which is either one of the parameters IDs in which case the \n",
    "patient belongs to the 1-D grid associated with that parameter. Or 'label' is 'multi' in which case the patient belongs \n",
    "to the \"exploratory\" Vpop where all parameters vary simultaneously.\n",
    "\"\"\"\n",
    "patient_map = {}\n",
    "for param_name, id_set in patient_groups.items():\n",
    "    for patient_id in id_set:\n",
    "        patient_map[patient_id] = param_name\n",
    "for patient in exploratory_vpop:\n",
    "    patient_map[patient[\"patientIndex\"]] = \"multi\"\n",
    "\n",
    "# We merge both Vpops, we will later use 'patient_map' to split the Vpop results\n",
    "vpop = {\"patients\": tensor_vpop + exploratory_vpop}\n",
    "\n",
    "\n",
    "response = jinko.make_request(\n",
    "    path=\"/core/v2/vpop_manager/vpop\",\n",
    "    method=\"POST\",\n",
    "    json=vpop,\n",
    "    options={\n",
    "        \"name\": \"Exploratory Vpop\",\n",
    "        \"folder_id\": folder_id,\n",
    "    },\n",
    ")\n",
    "project_item_info = jinko.get_project_item_info_from_response(response)\n",
    "vpop_core_item_id = project_item_info[\"coreItemId\"][\"id\"]\n",
    "vpop_snapshot_id = project_item_info[\"coreItemId\"][\"snapshotId\"]\n",
    "\n",
    "print(f\"Generated a Vpop of {len(vpop[\"patients\"])} patients\")\n",
    "print(f\"Resource link: {jinko.get_project_item_url_from_response(response)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30339e67",
   "metadata": {},
   "source": [
    "## Step 4: Create or update the exploratory Trial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b2aeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_data = {\n",
    "    \"computationalModelId\": calibration_inputs[\"computationalModelId\"],\n",
    "    \"protocolDesignId\": calibration_inputs[\"protocolDesignId\"],\n",
    "    \"vpopId\": {\"coreItemId\": vpop_core_item_id, \"snapshotId\": vpop_snapshot_id},\n",
    "    \"dataTableDesigns\": calibration_inputs[\"dataTableDesigns\"],\n",
    "    \"solvingOptions\": calibration_inputs[\"solvingOptions\"],\n",
    "    \"scoringDesignId\": calibration_inputs[\"scoringDesignId\"],\n",
    "}\n",
    "\n",
    "# If there's already a Trial ID in the cache, create a new version of the same Trial instead of creating a new one\n",
    "# Restart the notebook kernel when changing calibrations.\n",
    "if \"trial_core_item_id\" in globals() and \"trial_snapshot_id\" in globals():\n",
    "    response = jinko.make_request(\n",
    "        path=f\"/core/v2/trial_manager/trial/{trial_core_item_id}/snapshots/{trial_snapshot_id}\",\n",
    "        method=\"PATCH\",\n",
    "        json=trial_data,\n",
    "    )\n",
    "else:\n",
    "    response = jinko.make_request(\n",
    "        path=\"/core/v2/trial_manager/trial\",\n",
    "        method=\"POST\",\n",
    "        json=trial_data,\n",
    "        options={\n",
    "            \"name\": f\"UQ for Calibration {calibration_short_id}\",\n",
    "            \"folder_id\": folder_id,\n",
    "        },\n",
    "    )\n",
    "\n",
    "project_item_info = jinko.get_project_item_info_from_response(response)\n",
    "trial_core_item_id = project_item_info[\"coreItemId\"][\"id\"]\n",
    "trial_snapshot_id = project_item_info[\"coreItemId\"][\"snapshotId\"]\n",
    "\n",
    "print(f\"Resource link: {jinko.get_project_item_url_from_response(response)}\")\n",
    "\n",
    "trial_inputs = jinko.make_request(\n",
    "    f\"/core/v2/trial_manager/trial/{trial_core_item_id}/snapshots/{trial_snapshot_id}\",\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2d06d5",
   "metadata": {},
   "source": [
    "## Step 5: Run and monitor the trial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d46eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://doc.jinko.ai/api/#/paths/core-v2-trial_manager-trial-trialId--snapshots--trialIdSnapshot--run/post\n",
    "response = jinko.make_request(\n",
    "    path=f\"/core/v2/trial_manager/trial/{trial_core_item_id}/snapshots/{trial_snapshot_id}/run\",\n",
    "    method=\"POST\",\n",
    ")\n",
    "jinko.monitor_trial_until_completion(trial_core_item_id, trial_snapshot_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b42c5d",
   "metadata": {},
   "source": [
    "## Step 6: Load and process the trial results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c67b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalars_summary = jinko.get_trial_scalars_summary(\n",
    "    trial_core_item_id, trial_snapshot_id, print_summary=True\n",
    ")\n",
    "\n",
    "# Find baseline inputs, in other words the patients descriptors which were calibrated\n",
    "patientDescriptors = [f\"{p}.tmin\" for p in parameters]\n",
    "\n",
    "print(\"List of patient descriptors:\\n\", patientDescriptors, \"\\n\")\n",
    "\n",
    "resultScalars = list(objectives.keys()) + [\"optimizationWeightedScore\"]\n",
    "\n",
    "print(\"List of result scalars:\\n\", resultScalars, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f87ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfScalars = jinko.get_trial_scalars_as_dataframe(\n",
    "    trial_core_item_id, trial_snapshot_id, scalar_ids=resultScalars\n",
    ")\n",
    "dfDescriptors = jinko.get_trial_scalars_as_dataframe(\n",
    "    trial_core_item_id, trial_snapshot_id, scalar_ids=patientDescriptors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5796091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Filter the results by keeping only patients whose 'optimizationWeightedScore' is GREATER THAN a given threshold\n",
    "\"\"\"\n",
    "\n",
    "weightedScoreThreshold = -1e33\n",
    "\n",
    "\n",
    "# Renaming and filtering some columns\n",
    "dfScalars = dfScalars.rename(columns={\"value\": \"likelihood\", \"scalarId\": \"score\"}).drop(\n",
    "    columns=[\"unit\", \"armId\"]\n",
    ")\n",
    "# handle scalar errors\n",
    "dfScalars[\"likelihood\"] = pd.to_numeric(\n",
    "    dfScalars[\"likelihood\"], downcast=\"float\", errors=\"coerce\"\n",
    ")\n",
    "\n",
    "print(\"\\nRaw scalar results data (first rows):\\n\")\n",
    "display(dfScalars.head())\n",
    "\n",
    "# Renaming and filtering\n",
    "dfDescriptors = dfDescriptors.rename(\n",
    "    columns={\"value\": \"parameterValue\", \"scalarId\": \"parameterName\"}\n",
    ").drop(columns=[\"unit\", \"armId\"])\n",
    "# Remove the `.tmin` suffix to parameter names\n",
    "dfDescriptors[\"parameterName\"] = dfDescriptors[\"parameterName\"].apply(\n",
    "    lambda t: t.removesuffix(\".tmin\")\n",
    ")\n",
    "print(\"\\nPatients descriptors (first rows):\\n\")\n",
    "display(dfDescriptors.head())\n",
    "\n",
    "# Filter on optimizationWeightedScore\n",
    "dfPatients = dfScalars.loc[\n",
    "    (dfScalars[\"score\"] == \"optimizationWeightedScore\")\n",
    "    & (dfScalars[\"likelihood\"] > weightedScoreThreshold)\n",
    "][\"patientId\"]\n",
    "print(f\"\\nNumber of selected patients: {len(dfPatients)}\")\n",
    "\n",
    "# Final data set\n",
    "# Merge the scalars and descriptors\n",
    "dfFull = dfScalars.merge(dfDescriptors, how=\"inner\", on=[\"patientId\"])\n",
    "# Filter the data set\n",
    "dfFull = dfFull.loc[dfFull[\"patientId\"].isin(dfPatients)]\n",
    "# Apply a transformation to likelihood\n",
    "dfFull[\"likelihoodTrans\"] = dfFull[\"likelihood\"].apply(lambda y: np.exp(y - 1))\n",
    "\n",
    "# Label the rows depending on whether they belong to the \"tensorized\" sub-vpop or\n",
    "# the \"exploratory\" sub-vpop\n",
    "dfFull[\"label\"] = dfFull[\"patientId\"].apply(lambda patient_id: patient_map[patient_id])\n",
    "\n",
    "print(\"\\nMerged data set first 5 rows:\\n\")\n",
    "display(dfFull.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a09a50",
   "metadata": {},
   "source": [
    "## Step 7: Generate the uncertainty quantification report\n",
    "\n",
    "This section generates a grid of individual scatter plots. Each plot shows an individual score value in the y-axis against an individual parameter value in the x-axis.\n",
    "\n",
    "The rows of the grid correspond to different scores used in the calibration, with the first row being the `optimizationWeightedScore`.\n",
    "\n",
    "The columns of the grid correspond to the different parameters in the calibration options. An option is available to display any of them with a log-scale.\n",
    "\n",
    "Each plot contains\n",
    "\n",
    "- A green-cross: this shows the position (parameter value - score value) of the best patient from the calibration\n",
    "- A red-line: this shows the results of the single parameter sweep, for each considered parameter (depending on the facet). This is a very rough estimation of the likelihood profile in the direction of one parameter.\n",
    "- A scatter plot of blue points: these are all the individual patients in the multi-dimensional exploration trial. This gives an idea of the shape of the likelihood landscape in multi-dimension. Keep in mind that this plot is affected by the curse of dimensionality, and if the required number of patients to precisely estimate the value of the likelihood increases exponentially with the number of parameters.\n",
    "\n",
    "Parameter identifiability is associated to the \"sharpness\" of the likelihood profile around the maximal value.\n",
    "\n",
    "![Output plot explained](resources/post_calibration_uq/explo-calib-uq.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9a6248",
   "metadata": {},
   "outputs": [],
   "source": [
    "figureHeight = 500\n",
    "figureWidth = 1000\n",
    "\n",
    "scoreList = list(dfFull.score.unique())\n",
    "scoreList.remove(\"optimizationWeightedScore\")\n",
    "scoreList = [\n",
    "    \"optimizationWeightedScore\"\n",
    "]  # + scoreList # uncomment to print all the calibration scores\n",
    "nbScores = len(scoreList)\n",
    "\n",
    "scoreListWrapped = [\"<br>\".join(textwrap.wrap(t, width=12)) for t in scoreList]\n",
    "display(scoreListWrapped)\n",
    "\n",
    "nbParams = len(parameters)\n",
    "parametersWrapped = [\"<br>\".join(textwrap.wrap(t, width=30)) for t in parameters]\n",
    "\n",
    "# Add here all params that should be displayed using a log scale\n",
    "# By default, all params that had a custom range in log are selected\n",
    "logParams = [p for p in custom_range if custom_range[p][\"logscale\"] == True]\n",
    "\n",
    "fig = make_subplots(\n",
    "    len(scoreList),\n",
    "    len(parameters),\n",
    "    shared_xaxes=True,\n",
    "    shared_yaxes=True,\n",
    "    horizontal_spacing=0.01,\n",
    "    vertical_spacing=0.01,\n",
    "    column_titles=parametersWrapped,\n",
    "    row_titles=scoreListWrapped,\n",
    ")\n",
    "for i in range(nbScores):\n",
    "    for j in range(nbParams):\n",
    "        df = dfFull.loc[\n",
    "            (dfFull[\"parameterName\"] == parameters[j])\n",
    "            & (dfFull[\"score\"] == scoreList[i])\n",
    "        ]\n",
    "        scatter_df = df.loc[df[\"label\"] == \"multi\"]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=scatter_df[\"parameterValue\"],\n",
    "                y=scatter_df[\"likelihoodTrans\"],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=4, color=\"darkblue\", opacity=0.5),\n",
    "                hoverinfo=\"none\",\n",
    "            ),\n",
    "            row=i + 1,\n",
    "            col=j + 1,\n",
    "        )\n",
    "        mono_df = df.loc[df[\"label\"] == parameters[j]].sort_values(by=\"parameterValue\")\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=mono_df[\"parameterValue\"],\n",
    "                y=mono_df[\"likelihoodTrans\"],\n",
    "                line_width=3,\n",
    "                line_color=\"red\",\n",
    "            ),\n",
    "            row=i + 1,\n",
    "            col=j + 1,\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[best_patient_parameters[f\"{parameters[j]}.tmin\"]],\n",
    "                y=[best_patient_scores_trans[scoreList[i]]],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(\n",
    "                    size=10,\n",
    "                    color=\"white\",\n",
    "                    symbol=\"cross\",\n",
    "                    line=dict(color=\"green\", width=1),\n",
    "                ),\n",
    "            ),\n",
    "            row=i + 1,\n",
    "            col=j + 1,\n",
    "        )\n",
    "        if parameters[j] in logParams:\n",
    "            fig.update_xaxes(type=\"log\", row=i + 1, col=j + 1, tickformat=\".1e\")\n",
    "fig.update_annotations(font_size=10)\n",
    "fig.update_layout(\n",
    "    font=dict(size=10),\n",
    "    showlegend=False,\n",
    "    width=figureWidth,\n",
    "    height=figureHeight,\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "fig.show(renderer=\"jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1937365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional plot: pairwise correlation plots for calibrated parameters\n",
    "\n",
    "# Add here all params that should be displayed using a log scale\n",
    "# By default, all params that had a custom range in log are selected\n",
    "logParams = [p for p in custom_range if custom_range[p][\"logscale\"] == True]\n",
    "\n",
    "figureHeight = 600\n",
    "figureWidth = 600\n",
    "\n",
    "fig = make_subplots(\n",
    "    len(parameters),\n",
    "    len(parameters),\n",
    "    shared_xaxes=True,\n",
    "    shared_yaxes=True,\n",
    "    horizontal_spacing=0.01,\n",
    "    vertical_spacing=0.01,\n",
    "    column_titles=parametersWrapped,\n",
    "    row_titles=parametersWrapped,\n",
    ")\n",
    "for i in range(nbParams):  # iterating over rows\n",
    "    for j in range(nbParams):  # iterating over columns\n",
    "        if i == j:  # do not plot anything on the diagonal\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=[], y=[]),\n",
    "                row=i + 1,\n",
    "                col=j + 1,\n",
    "            )\n",
    "        else:\n",
    "            df_j = dfFull.loc[\n",
    "                (dfFull[\"parameterName\"] == parameters[j])\n",
    "                & (dfFull[\"score\"] == \"optimizationWeightedScore\")\n",
    "                & (dfFull[\"label\"] == \"multi\")\n",
    "            ]\n",
    "            df_j = df_j[[\"parameterValue\", \"likelihoodTrans\", \"patientId\"]].rename(\n",
    "                columns={\"parameterValue\": parameters[j]}\n",
    "            )\n",
    "            df_i = dfFull.loc[\n",
    "                (dfFull[\"parameterName\"] == parameters[i])\n",
    "                & (dfFull[\"score\"] == \"optimizationWeightedScore\")\n",
    "                & (dfFull[\"label\"] == \"multi\")\n",
    "            ]\n",
    "            df_i = df_i[[\"parameterValue\", \"patientId\"]].rename(\n",
    "                columns={\"parameterValue\": parameters[i]}\n",
    "            )\n",
    "\n",
    "            scatter_df = df_j.merge(df_i, how=\"inner\", on=[\"patientId\"])\n",
    "            scatter_df = scatter_df.sort_values(by=\"likelihoodTrans\")\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=scatter_df[parameters[j]],  # x axis is the column parameter\n",
    "                    y=scatter_df[parameters[i]],  # y axis is the row parameter\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(\n",
    "                        size=6,\n",
    "                        color=scatter_df[\"likelihoodTrans\"],\n",
    "                        opacity=(\n",
    "                            scatter_df[\"likelihoodTrans\"]\n",
    "                            / scatter_df[\"likelihoodTrans\"].max()\n",
    "                        ),\n",
    "                        coloraxis=\"coloraxis1\",\n",
    "                    ),\n",
    "                    hoverinfo=\"none\",\n",
    "                ),\n",
    "                row=i + 1,\n",
    "                col=j + 1,\n",
    "            )\n",
    "\n",
    "        # Set the log scale for x and / or y axes\n",
    "        if parameters[j] in logParams:\n",
    "            fig.update_xaxes(type=\"log\", row=i + 1, col=j + 1, tickformat=\".1e\")\n",
    "        if parameters[i] in logParams:\n",
    "            fig.update_yaxes(type=\"log\", row=i + 1, col=j + 1, tickformat=\".1e\")\n",
    "fig.update_annotations(font_size=10)\n",
    "fig.update_xaxes(tickangle=40)\n",
    "fig.update_coloraxes(\n",
    "    colorbar_title=\"Likelihood\",\n",
    "    cmin=0,\n",
    "    cmax=0.9,\n",
    "    colorscale=\"Rainbow\",\n",
    "    colorbar_thickness=15,\n",
    ")\n",
    "fig.update_layout(\n",
    "    font=dict(size=10),\n",
    "    showlegend=False,\n",
    "    # coloraxis_showscale=True,\n",
    "    width=figureWidth,\n",
    "    height=figureHeight,\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "fig.show(renderer=\"jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a5626c",
   "metadata": {},
   "source": [
    "## Approximate the Hessian matrix of the likelihood\n",
    "\n",
    "For the \"best patient\" to be at a local maximum likelihood, all Hessian matrix eigenvalues must be negative and the gradient must be zero.\n",
    "See the \"hessian_matrix.ipynb\" cookbook for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d137da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Massage the data such that we have a matrix of samples of size (N, dim) and a vector of likelihood values of size N\n",
    "df = dfFull.loc[\n",
    "    (dfFull[\"label\"] == \"multi\") & (dfFull[\"score\"] == \"optimizationWeightedScore\")\n",
    "]\n",
    "pivot_df = df.pivot(index=\"patientId\", columns=\"parameterName\", values=\"parameterValue\")\n",
    "\n",
    "likelihood_df = df.groupby(by=\"patientId\")[\"likelihood\"].first().to_frame()\n",
    "likelihood_df_sorted = likelihood_df.sort_values(by=\"patientId\", ascending=False)\n",
    "pivot_df_sorted = pivot_df.loc[likelihood_df_sorted.index]\n",
    "unscaled_samples = pivot_df_sorted.values\n",
    "\n",
    "# Scale the samples\n",
    "# The centroid is the point in the paramter space corresponding to the MLE\n",
    "unscaled_centroid = np.array(\n",
    "    [\n",
    "        best_patient_parameters[f\"{param_name}.tmin\"]\n",
    "        for param_name in pivot_df_sorted.columns\n",
    "    ]\n",
    ")\n",
    "unscaled_x = np.vstack((unscaled_samples, unscaled_centroid))\n",
    "scaled_x = minmax_scale(unscaled_x, feature_range=(-0.1, 0.1))\n",
    "samples, centroid = scaled_x[:-1, :], scaled_x[-1, :]\n",
    "likelihood_values = likelihood_df_sorted.values\n",
    "N, dim = samples.shape\n",
    "distance_to_centroid = np.linalg.norm(samples - centroid, ord=2, axis=1)\n",
    "x_order = np.argsort(distance_to_centroid)\n",
    "\n",
    "\n",
    "# This function fits a quadratic model f(X) = X^TAX + BX + C to the likelihood\n",
    "# The Hessian matrix of f(X) is 2A and its gradient is 2AX + B\n",
    "def fit_quadratic_model(X, y):\n",
    "    poly_reg = PolynomialFeatures(degree=2)\n",
    "    quadratic_monomials = poly_reg.fit_transform(X)\n",
    "\n",
    "    regressor = LinearRegression(\n",
    "        fit_intercept=False\n",
    "    )  # intercept is already accounted for by quadratic_monomials\n",
    "    regressor.fit(quadratic_monomials, y)\n",
    "    # The r2 score will be used to quantify the goodness of fit of the quadratic model\n",
    "    r2_score = regressor.score(quadratic_monomials, y)\n",
    "\n",
    "    # assemble the Hessian matrix and gradient\n",
    "    H = np.zeros((dim, dim))\n",
    "    g = np.zeros((dim,))\n",
    "    regressor_coefs = regressor.coef_.flatten()\n",
    "    for r, row in enumerate(poly_reg.powers_):\n",
    "        c = regressor_coefs[r]\n",
    "        if np.sum(row) == 2:\n",
    "            (nonzeros,) = np.nonzero(row)\n",
    "            if len(nonzeros) == 1:\n",
    "                i = nonzeros[0]\n",
    "                H[i, i] += c  # dfdxidxi = aii\n",
    "                g[i] += 2 * c * centroid[i]  #\n",
    "            elif len(nonzeros) == 2:\n",
    "                i, j = nonzeros[0], nonzeros[1]\n",
    "                H[i, j] += c / 2\n",
    "                H[j, i] += c / 2\n",
    "                g[i] += c * centroid[j]\n",
    "                g[j] += c * centroid[i]\n",
    "            else:\n",
    "                print(f\"Hey that's weird: {row}\")\n",
    "        elif np.sum(row) == 1:\n",
    "            (nonzeros,) = np.nonzero(row)\n",
    "            if len(nonzeros) == 1:\n",
    "                i = nonzeros[0]\n",
    "                g[i] += c\n",
    "            else:\n",
    "                print(f\"Hey that's weird: {row}\")\n",
    "        else:  # degree 0 does not contribute to the Hessian\n",
    "            continue\n",
    "    return H, g, r2_score\n",
    "\n",
    "\n",
    "H, gradient, r2_score = fit_quadratic_model(samples, likelihood_values)\n",
    "eigen_decomposition = np.linalg.eig(H)\n",
    "print(f\"Hessian matrix:\\n {H}\")\n",
    "print(f\"Gradient:\\n {gradient}\")\n",
    "print(f\"Quadratic fit R2 score = {r2_score}\")\n",
    "h_df = pd.DataFrame(\n",
    "    data=np.hstack(\n",
    "        (\n",
    "            np.reshape(eigen_decomposition.eigenvalues, (dim, 1)),\n",
    "            eigen_decomposition.eigenvectors.T,\n",
    "        )\n",
    "    ),\n",
    "    columns=[\"eigenValue\"] + [f\"x{k+1}\" for k in range(dim)],\n",
    ")\n",
    "h_df = h_df.sort_values(by=\"eigenValue\", ascending=True)\n",
    "h_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa75154",
   "metadata": {},
   "source": [
    "## Analyze the quality of the approximation as a function of the number of samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba81c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many points (minimum) do we need to fit a quadratic model?\n",
    "poly_reg = PolynomialFeatures(degree=2)\n",
    "quadratic_monomials = poly_reg.fit_transform(samples)\n",
    "min_n = quadratic_monomials.shape[1]\n",
    "if N < min_n:\n",
    "    raise Exception(\n",
    "        f\"In dimension {dim}, one needs at least {min_n} points to fit a quadratic model, got {N} instead\"\n",
    "    )\n",
    "\n",
    "# Compute and plot the Hessian eigen values, gradient and r2 score for each value of n\n",
    "n_vector = np.arange(min_n, N + 1, 1)\n",
    "eigenvalues = []\n",
    "r2_scores = []\n",
    "gradients = []\n",
    "for n in n_vector:\n",
    "    x = samples[x_order[:n], :]\n",
    "    y = likelihood_values[x_order[:n]]\n",
    "    H, gradient, r2_score = fit_quadratic_model(x, y)\n",
    "    eigen_decomposition = np.linalg.eig(H)\n",
    "    eigenvalues.append(np.sort(eigen_decomposition.eigenvalues))\n",
    "    r2_scores.append(r2_score)\n",
    "    gradients.append(np.linalg.norm(gradient, ord=2))\n",
    "eigenvalues = np.array(eigenvalues)\n",
    "n_vector\n",
    "fig = go.Figure(\n",
    "    go.Scatter(x=n_vector, y=eigenvalues[:, 0], name=f\"lambda1\", mode=\"lines\")\n",
    ")\n",
    "for k in range(1, dim):\n",
    "    fig.add_scatter(x=n_vector, y=eigenvalues[:, k], mode=\"lines\", name=f\"lambda{k+1}\")\n",
    "fig.update_layout(width=950, xaxis_title=\"n\", yaxis_title=\"Hessian matrix eigen values\")\n",
    "fig.show(renderer=\"jpeg\")\n",
    "\n",
    "fig = go.Figure(go.Scatter(x=n_vector, y=gradients, mode=\"lines\"))\n",
    "fig.update_layout(width=950, xaxis_title=\"n\", yaxis_title=\"norm2(gradient)\")\n",
    "fig.show(renderer=\"jpeg\")\n",
    "\n",
    "fig = go.Figure(go.Scatter(x=n_vector, y=r2_scores, mode=\"lines\"))\n",
    "fig.update_layout(width=950, xaxis_title=\"n\", yaxis_title=\"quadratic fit r2 score\")\n",
    "fig.show(renderer=\"jpeg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
