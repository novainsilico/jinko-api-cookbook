{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d54b1324-9c03-46f7-b920-5ff92c95123e",
   "metadata": {},
   "source": [
    "# Quantifying uncertainty in simulation results\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The purpose of this cookbook is to demonstrate how one can compute and plot the 95% percentile prediction interval (PPI) of the mean for multiple time series at once.\n",
    "\n",
    "To compute a PPI for a given summary metric (e.g.: the mean), a given number of randomly samples of size n are drawn from a larger population. The metric of interest is computed for each sample, therefore allowing to estimate the empirical distribution of the latter. The 2.5% and 97.5% percentiles are then estimated.\n",
    "\n",
    "The PPI is a relevant metric to assess the degree of uncertainty embedded in the model and eventually to compare it to the uncertainty observed in a real-life setting. Indeed, standard confidence intervals are not well suited for the in silico context as they tend to get very narrow as the Virtual Population (VP) size increases. On the other hand, PPI allows to define a sample size (in the case where the VP is much larger than a real-life clinical trial, one can use the same sample size as the one used for real life observations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138dd704-b7d0-42f4-867c-ea4f80c1adb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jinko specifics imports & initialization\n",
    "# Please fold this section and do not edit it\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../lib\")\n",
    "import jinko_helpers as jinko\n",
    "\n",
    "# Connect to Jinko (see README.md for more options)\n",
    "\n",
    "jinko.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9944f60-72f7-40ae-b85a-af6ebbdcae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cookbook specifics imports\n",
    "import io\n",
    "import json\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import zipfile\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Cookbook specifics constants:\n",
    "# put here the constants that are specific to your cookbook like\n",
    "# the reference to the Jinko items, the name of the model, etc.\n",
    "\n",
    "# @param {\"name\":\"trialId\", \"type\": \"string\"}\n",
    "# trial short id can be retrieved in the url, pattern is `https://jinko.ai/<trail_sid>`\n",
    "trial_sid = \"tr-9Bid-BL1I\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6829c0-29c3-4ac7-a0ec-68f5d542c48c",
   "metadata": {},
   "source": [
    "## Step 1: Loading the trial and getting the last completed version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f24608-a47d-4339-b683-25e3a041f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert short id to core item id\n",
    "trial_core_item_id = jinko.get_core_item_id(trial_sid, 1)\n",
    "\n",
    "# List all trial versions\n",
    "# https://doc.jinko.ai/api/#/paths/core-v2-trial_manager-trial-trialId--status/get\n",
    "response = jinko.make_request(\n",
    "    f'/core/v2/trial_manager/trial/{trial_core_item_id[\"id\"]}/status'\n",
    ")\n",
    "versions = response.json()\n",
    "\n",
    "# Get the latest completed version\n",
    "try:\n",
    "    latest_completed_version = next(\n",
    "        (item for item in versions if item[\"status\"] == \"completed\"), None\n",
    "    )\n",
    "    if latest_completed_version is None:\n",
    "        raise Exception(\"No completed trial version found\")\n",
    "    else:\n",
    "        print(\n",
    "            \"Successfully fetched this simulation:\\n\",\n",
    "            json.dumps(latest_completed_version, indent=1),\n",
    "        )\n",
    "        simulation_id = latest_completed_version[\"simulationId\"]\n",
    "        trial_core_item_id = simulation_id[\"coreItemId\"]\n",
    "        trial_snapshot_id = simulation_id[\"snapshotId\"]\n",
    "except Exception as e:\n",
    "    print(f\"Error processing trial versions: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a99d77a-e6aa-4f7f-97e7-287759b73249",
   "metadata": {},
   "source": [
    "## Step 2: Displaying a summary of the data content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8fa188-1157-41a5-b3a6-0437ba55d482",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_summary = jinko.get_trial_scalars_summary(trial_core_item_id, trial_snapshot_id, print_summary=True)\n",
    "\n",
    "# Extracting arm names\n",
    "arm_names = response_summary['arms']\n",
    "\n",
    "# Store the list of scenario descriptors fetch them\n",
    "scenario_descriptors = [\n",
    "    scalar[\"id\"]\n",
    "    for scalar in (response_summary[\"scalars\"] + response_summary[\"categoricals\"])\n",
    "    if \"ScenarioOverride\" in scalar[\"type\"][\"labels\"]\n",
    "]\n",
    "print(\"List of scenario overrides:\\n\", scenario_descriptors, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ccf1f1-476f-4f27-9089-47c00d8deacd",
   "metadata": {},
   "source": [
    "## Step 3: Retrieving time series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22b8ac0-11d5-461e-a0bf-43d1d2feb15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the time series to retrieve\n",
    "time_series_ids = [\"Blood.Drug\", \"Tumor.CancerCell\"]\n",
    "\n",
    "try:\n",
    "    print(\"Retrieving time series data...\")\n",
    "    response = jinko.make_request(\n",
    "        \"/core/v2/result_manager/trial/%s/snapshots/%s/timeseries/download\" % (\n",
    "            trial_core_item_id, trial_snapshot_id\n",
    "        ),\n",
    "        method='POST',\n",
    "        json={\n",
    "            \"timeseries\": {ts: arm_names for ts in time_series_ids},\n",
    "        },\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        print(\"Time series data retrieved successfully.\")\n",
    "        archive = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "        filename = archive.namelist()[0]\n",
    "        print(f\"Extracted time series file: {filename}\")\n",
    "        csv_time_series = archive.read(filename).decode(\"utf-8\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"Failed to retrieve time series data: {response.status_code} - {response.reason}\"\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "except Exception as e:\n",
    "    print(f\"Error during time series retrieval or processing: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b2d09b-f452-4a63-9e5e-25fa8fc268da",
   "metadata": {},
   "source": [
    "## Step 4: Post-processing the time series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172476bc-21e6-4431-9c01-1a2a7c67cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading timeseries into a dataframe\n",
    "df_time_series = pd.read_csv(io.StringIO(csv_time_series))\n",
    "print(\"Raw timeseries data (first rows): \\n\")\n",
    "display(df_time_series.head())\n",
    "\n",
    "# Count the number of observations per time point\n",
    "counts = df_time_series[\"Time\"].value_counts()\n",
    "\n",
    "# Check if all time points have the same number of observations\n",
    "all_equal = counts.nunique() == 1\n",
    "\n",
    "if all_equal:\n",
    "    print(\"All time points have the same number of observations.\")\n",
    "else:\n",
    "    print(f\"Time points have varying numbers of observations:\\n{counts.value_counts()}\")\n",
    "\n",
    "n_patients = len(df_time_series[\"Patient Id\"].unique())\n",
    "print(f\"Successfully loaded {n_patients} patients.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304d92f1-fbf5-45ba-bc3b-8bb6ab1bd9d2",
   "metadata": {},
   "source": [
    "## Step 5: Computing mean value by time point, for each arm and each descriptor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae04da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_means_grouped = (\n",
    "    df_time_series\n",
    "    .groupby([\"Arm\", \"Descriptor\", \"Time\"])[\"Value\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"Value\":\"Mean\"})\n",
    ")\n",
    "display(df_means_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71b0d28-ffae-4d1a-8705-0e52ff7e4654",
   "metadata": {},
   "source": [
    "## Step 6: Computing the 95% percentiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8549cca3-596f-4e47-87af-b82314d7ea14",
   "metadata": {},
   "source": [
    "### Defining useful functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2521e8-9f01-4ec4-9508-dbb288e47acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for each quantile of interest\n",
    "def q_0025(x):\n",
    "    return x.quantile(0.025)\n",
    "\n",
    "def q_975(x):\n",
    "    return x.quantile(0.975)\n",
    "\n",
    "# Function to sample the boostrapping groups (sampling over individual patients)\n",
    "def generate_subsample_groups(data, num_subsamples, sample_size):\n",
    "    patient_ids=data[\"Patient Id\"].unique()\n",
    "    npatients=len(patient_ids)\n",
    "    groups=patient_ids[np.random.randint(npatients,size=(num_subsamples,sample_size))]\n",
    "    return groups\n",
    "\n",
    "# A function to compute the mean over each group of the data frame\n",
    "# Defined in a separate module so that mutiprocessing accepts it\n",
    "def compute_group_mean(group):\n",
    "    temp_df = df_time_series.loc[df_time_series[\"Patient Id\"].isin(group)].groupby([\"Descriptor\", \"Arm\", \"Time\"])[\"Value\"].mean().explode(0).reset_index().rename(columns={\"Value\":\"Subsample_Mean\"})\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e95bdbe",
   "metadata": {},
   "source": [
    "### Running the bootstrapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a7c0de-5923-4f7c-bbfd-61ae0e1ffa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of subsamples and sample size\n",
    "num_subsamples = 500\n",
    "sample_size = 50\n",
    "\n",
    "# Define the groups\n",
    "groups = generate_subsample_groups(df_time_series,num_subsamples,sample_size)\n",
    "\n",
    "# For each group, compute the mean over the filtered and grouped data frame\n",
    "# This computation is parallelized using the multiprocessing library (https://docs.python.org/3/library/multiprocessing.html)\n",
    "pool = Pool()\n",
    "dfs = pool.map(compute_group_mean,groups)\n",
    "df_subsample_means=pd.concat(dfs)\n",
    "\n",
    "\n",
    "# Computing percentiles\n",
    "df_percentiles_grouped = (\n",
    "    df_subsample_means.groupby([\"Arm\", \"Descriptor\", \"Time\"])\n",
    "    .agg(LoBound=('Subsample_Mean',q_0025),HiBound=('Subsample_Mean',q_975))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merging the two data frames together\n",
    "df_ppi = pd.merge(\n",
    "    df_means_grouped, df_percentiles_grouped, on=[\"Arm\", \"Descriptor\", \"Time\"]\n",
    ")\n",
    "display(df_ppi.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91983fa-1eb6-4ca6-a06a-3ba17cd600a8",
   "metadata": {},
   "source": [
    "# Step 7: Plotting the outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764188f3-023e-4729-b331-bb137b498e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating subplots\n",
    "unique_variables = df_ppi[\"Descriptor\"].unique()\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=len(unique_variables),\n",
    "    shared_yaxes=False,\n",
    "    subplot_titles=unique_variables,\n",
    ")\n",
    "\n",
    "## Defining colors for different arms\n",
    "palette = px.colors.qualitative.Plotly\n",
    "\n",
    "## Creating a dictionary to map each arm to a color\n",
    "unique_arm = df_ppi[\"Arm\"].unique()\n",
    "color_map = {\n",
    "    category: palette[i % len(palette)] for i, category in enumerate(unique_arm)\n",
    "}\n",
    "\n",
    "\n",
    "## Looping through each descriptor and adding traces for mean, lower bound, and upper bound stratified by arm\n",
    "for i, group in enumerate(unique_variables):\n",
    "    group_df = df_ppi[df_ppi[\"Descriptor\"] == group]\n",
    "\n",
    "    for arm in unique_arm:\n",
    "        subset = group_df[group_df[\"Arm\"] == arm]\n",
    "        x=subset[\"Time\"]\n",
    "        xrev=x[::-1]\n",
    "        yHigh=subset[\"HiBound\"]\n",
    "        yLowRev=subset[\"LoBound\"][::-1]\n",
    "        # Add the mean line (plain line)\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=subset[\"Time\"],\n",
    "                y=subset[\"Mean\"],\n",
    "                mode=\"lines\",\n",
    "                name=f\"{group} {arm} Mean\",\n",
    "                line=dict(color=color_map[arm]),\n",
    "            ),\n",
    "            row=1,\n",
    "            col=i + 1,\n",
    "        )\n",
    "\n",
    "        # Add the ribbon plot of the prediction precidion interval\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=pd.concat([x,xrev]),\n",
    "                y=pd.concat([yHigh,yLowRev]),\n",
    "                mode=\"lines\",\n",
    "                fill=\"toself\",\n",
    "                opacity=0.5,\n",
    "                name=f\"{group} {arm} 95% PPI\",\n",
    "                line=dict(color=color_map[arm], width=0),\n",
    "            ),\n",
    "            row=1,\n",
    "            col=i + 1,\n",
    "        )\n",
    "\n",
    "## Updating the layout\n",
    "fig.update_layout(\n",
    "    title=\"Mean and Bootstrapped 95% Prediction Interval Stratified by Variable and Arm\",\n",
    "    xaxis_title=\"X-axis\",\n",
    "    yaxis_title=\"Values\",\n",
    "    legend_title=\"Legend\",\n",
    ")\n",
    "\n",
    "## Show the plot\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jinko-api-cookbook-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
