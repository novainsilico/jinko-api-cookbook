{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical trial design optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cookbook specifics imports\n",
    "import jinko_helpers as jinko\n",
    "import json\n",
    "import io\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import scipy.stats as st\n",
    "from sklearn.neighbors import KNeighborsRegressor, KDTree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import zipfile\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining general parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jinko trial short ID, URL pattern is \"https://jinko.ai/{trial_sid}\"\n",
    "trial_sid = \"tr-HLRF-b0zW\"\n",
    "# Outcome ID\n",
    "outcome_name = \"tumorBurdenChangeFromBaseline.tend\"\n",
    "# control and treated arm IDs\n",
    "control_arm_id = \"sc-1-10\"\n",
    "treated_arm_id = \"iv-1-10\"\n",
    "\n",
    "\"\"\"\n",
    "Sample size parameters\n",
    "\"\"\"\n",
    "alpha = 0.05\n",
    "beta = 0.2\n",
    "\n",
    "\"\"\" \n",
    "Features sorted by descending order of importance (as per tornado or Random Forest analysis for instance)\n",
    "See \"sensitivity_analysis.ipynb\"\n",
    "\"\"\"\n",
    "all_features = [\n",
    "    \"initialTumorBurden.tmin\",\n",
    "    \"ec50Drug.tmin\",\n",
    "    \"Blood.tmin\",\n",
    "    \"Tissue.tmin\",\n",
    "    \"kClearanceDrug.tmin\",\n",
    "]\n",
    "\n",
    "\"\"\" Minimal size of the filtered responder vpop\n",
    "(number of times the required sample size should be included in the responder vp in terms of nb of patients)\n",
    "Will be evaluated for each set of eligibility criteria\n",
    "\"\"\"\n",
    "min_resp_vp_size = 10\n",
    "\n",
    "\"\"\"\n",
    "Objective function weights\n",
    "\"\"\"\n",
    "# weight of gross efficacy\n",
    "efficacy_wt = 1\n",
    "# weight of standard deviation of gross efficacy\n",
    "efficacy_sd_wt = 2\n",
    "\n",
    "\"\"\"\n",
    "Boostrapping parameters\n",
    "\"\"\"\n",
    "## Number of bootstraps for gross efficacy dispersion estimation\n",
    "num_bootstraps = 50\n",
    "## Seed for all random processes\n",
    "cookbook_seed = 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions\n",
    "### Sample size computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of sample size computation formulas\n",
    "## Sample size for two independent samples, continuous outcome\n",
    "def sample_size_continuous_outcome(alpha, beta, diff_btw_groups, sd_outcome, dropout=0):\n",
    "    z_alpha = st.norm.ppf(1 - (alpha / 2))\n",
    "    z_beta = st.norm.ppf(1 - beta)\n",
    "\n",
    "    return (2 * ((z_alpha + z_beta) / (abs(diff_btw_groups) / sd_outcome)) ** 2) / (\n",
    "        1 - dropout\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function to be maximized\n",
    "def objective_function(\n",
    "    efficacy,\n",
    "    efficacy_sd,\n",
    "):\n",
    "    return (efficacy * efficacy_wt - efficacy_sd * efficacy_sd_wt) / (\n",
    "        efficacy_wt + efficacy_sd_wt\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading trial results from Jinko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jinko.initialize()\n",
    "\n",
    "# Convert short id to core item id\n",
    "trial_core_item_id = jinko.get_core_item_id(trial_sid, 1)\n",
    "\n",
    "# List all trial versions\n",
    "# https://doc.jinko.ai/api/#/paths/core-v2-trial_manager-trial-trialId--status/get\n",
    "response = jinko.make_request(\n",
    "    f'/core/v2/trial_manager/trial/{trial_core_item_id[\"id\"]}/status'\n",
    ")\n",
    "versions = response.json()\n",
    "\n",
    "# Get the latest completed version\n",
    "try:\n",
    "    latest_completed_version = next(\n",
    "        (item for item in versions if item[\"status\"] == \"completed\"), None\n",
    "    )\n",
    "    if latest_completed_version is None:\n",
    "        raise Exception(\"No completed trial version found\")\n",
    "    else:\n",
    "        simulation_id = latest_completed_version[\"simulationId\"]\n",
    "        trial_core_item_id = simulation_id[\"coreItemId\"]\n",
    "        trial_snapshot_id = simulation_id[\"snapshotId\"]\n",
    "except Exception as e:\n",
    "    print(f\"Error processing trial versions: {e}\")\n",
    "    raise\n",
    "\n",
    "# https://doc.jinko.ai/api/#/paths/core-v2-trial_manager-trial-trialId--snapshots--trialIdSnapshot--results_summary/get\n",
    "response = jinko.make_request(\n",
    "    f\"/core/v2/trial_manager/trial/{trial_core_item_id}/snapshots/{trial_snapshot_id}/results_summary\",\n",
    "    method=\"GET\",\n",
    ")\n",
    "response_summary = json.loads(response.content)\n",
    "\n",
    "# Retrieving scalar results\n",
    "json_data = {\n",
    "    \"trialId\": {\"coreItemId\": trial_core_item_id, \"snapshotId\": trial_snapshot_id}\n",
    "}\n",
    "\n",
    "# https://doc.jinko.ai/api/#/paths/core-v2-result_manager-scalars_summary/post\n",
    "response = jinko.make_request(\n",
    "    path=\"/core/v2/result_manager/trial_visualization\",\n",
    "    method=\"POST\",\n",
    "    json=json_data,\n",
    ")\n",
    "\n",
    "# https://doc.jinko.ai/api/#/paths/core-v2-result_manager-scalars_summary/post\n",
    "response = jinko.make_request(\n",
    "    path=\"/core/v2/result_manager/scalars_summary\",\n",
    "    method=\"POST\",\n",
    "    json={\n",
    "        \"select\": all_features + [outcome_name],\n",
    "        \"trialId\": latest_completed_version[\"simulationId\"],\n",
    "    },\n",
    ")\n",
    "archive = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "filename = archive.namelist()[0]\n",
    "\n",
    "csv_scalars = archive.read(filename).decode(\"utf-8\")\n",
    "\n",
    "scalars_dtf = pd.read_csv(io.StringIO(csv_scalars))\n",
    "print(\"Number of rows in the initial table:\", len(scalars_dtf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembling the dataframes of interest\n",
    "cross_arm_scalars = scalars_dtf.loc[scalars_dtf[\"armId\"] == \"crossArms\"].pivot(\n",
    "    index=\"patientId\", columns=\"scalarId\", values=\"value\"\n",
    ")\n",
    "control_arm_scalars = scalars_dtf.loc[scalars_dtf[\"armId\"] == control_arm_id].pivot(\n",
    "    index=\"patientId\", columns=\"scalarId\", values=\"value\"\n",
    ")\n",
    "treated_arm_scalars = scalars_dtf.loc[scalars_dtf[\"armId\"] == treated_arm_id].pivot(\n",
    "    index=\"patientId\", columns=\"scalarId\", values=\"value\"\n",
    ")\n",
    "control_arm_scalars = pd.merge(\n",
    "    left=cross_arm_scalars, right=control_arm_scalars, how=\"left\", on=\"patientId\"\n",
    ")\n",
    "treated_arm_scalars = pd.merge(\n",
    "    left=cross_arm_scalars, right=treated_arm_scalars, how=\"left\", on=\"patientId\"\n",
    ")\n",
    "\n",
    "print(f\"cross_arm_scalars table has {len(cross_arm_scalars)} rows\")\n",
    "print(f\"control_arm_scalars table has {len(control_arm_scalars)} rows\")\n",
    "print(f\"treated_arm_scalars table has {len(treated_arm_scalars)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial net efficacy and sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_control = control_arm_scalars[outcome_name].mean()\n",
    "mean_treated = treated_arm_scalars[outcome_name].mean()\n",
    "initial_net_efficacy = mean_control - mean_treated\n",
    "\n",
    "std_control = control_arm_scalars[outcome_name].std()\n",
    "std_treated = treated_arm_scalars[outcome_name].std()\n",
    "\n",
    "print(\n",
    "    f\"This trial has a net efficacy of {initial_net_efficacy:.3g} (standard deviation of outcome in control group = {std_control:.2g})\"\n",
    ")\n",
    "sample_size = math.ceil(\n",
    "    sample_size_continuous_outcome(alpha, beta, initial_net_efficacy, std_control)\n",
    ")\n",
    "print(f\"With beta = {beta}, this means a sample size of {sample_size} patients per group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting score proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the actual score function takes the average and standard deviation of the bootstrapped gross efficacy, this is \n",
    "rather costly to evaluate (because of the bootstraps).\n",
    "Here, we leverage the fact that:\n",
    "\n",
    "  1. the mean of the gross efficacy can be approximated by the mean of the absolute benefit \n",
    "  2. the variance of the gross efficacy can be approximated by '(var(outcome_control) + var(outcome_treated)) / sample_size'\n",
    "to compute a so-called \"score proxy\"\n",
    "\n",
    "\n",
    "Using a nearest-neighbor regression with \"number of neighbors = sample_size * min_resp_vp_size\", the Chebyshev norm and uniform weighting, \n",
    "we can compute a score proxy for each trial patient which approximates what the actual score sould be for \n",
    "a sub-population of size sample_size centered around each patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficacy_df = pd.merge(\n",
    "    control_arm_scalars, treated_arm_scalars[outcome_name], how=\"inner\", on=\"patientId\"\n",
    ")\n",
    "efficacy_df = efficacy_df.dropna()\n",
    "\n",
    "neigh_regressor = KNeighborsRegressor(\n",
    "    n_neighbors=sample_size * min_resp_vp_size, weights=\"uniform\", metric=\"chebyshev\"\n",
    ")\n",
    "X_unscaled = efficacy_df[all_features].to_numpy()\n",
    "feature_scaler = StandardScaler().fit(X_unscaled)\n",
    "X = feature_scaler.transform(X_unscaled)\n",
    "for z in [\"x\", \"y\"]:\n",
    "    neigh_regressor.fit(\n",
    "        X,\n",
    "        efficacy_df[f\"{outcome_name}_{z}\"].to_numpy(),\n",
    "    )\n",
    "    efficacy_df[f\"mu_{outcome_name}_{z}\"] = neigh_regressor.predict(X)\n",
    "\n",
    "    neigh_regressor.fit(\n",
    "        X,\n",
    "        (efficacy_df[f\"{outcome_name}_{z}\"].to_numpy()) ** 2,\n",
    "    )\n",
    "    efficacy_df[f\"mu_{outcome_name}_{z}2\"] = neigh_regressor.predict(X)\n",
    "    efficacy_df[f\"var_{outcome_name}_{z}\"] = (\n",
    "        efficacy_df[f\"mu_{outcome_name}_{z}2\"]\n",
    "        - efficacy_df[f\"mu_{outcome_name}_{z}\"] ** 2\n",
    "    )\n",
    "\n",
    "# score proxy where the mean of gross efficacy is approximated using the mean of absolute benefit\n",
    "# and the variance of gross efficacy is approximated as 'var(control) + var(treated) / sample_size'\n",
    "efficacy_df[\"score_proxy\"] = objective_function(\n",
    "    efficacy_df[f\"mu_{outcome_name}_x\"] - efficacy_df[f\"mu_{outcome_name}_y\"],\n",
    "    np.sqrt(efficacy_df[f\"var_{outcome_name}_x\"] + efficacy_df[f\"var_{outcome_name}_y\"])\n",
    "    / np.sqrt(sample_size),\n",
    ")\n",
    "min_score, max_score = (\n",
    "    efficacy_df[\"score_proxy\"].min(),\n",
    "    efficacy_df[\"score_proxy\"].max(),\n",
    ")\n",
    "print(f\"Min score proxy = {min_score:.3g}, max score proxy = {max_score:.3g}\")\n",
    "\n",
    "efficacy_df = efficacy_df.sort_values(by=\"score_proxy\")\n",
    "num_features = len(all_features)\n",
    "features_wrapped = [\"<br>\".join(textwrap.wrap(t, width=30)) for t in all_features]\n",
    "fig = make_subplots(\n",
    "    num_features,\n",
    "    num_features,\n",
    "    shared_xaxes=True,\n",
    "    shared_yaxes=False,\n",
    "    horizontal_spacing=0.03,\n",
    "    vertical_spacing=0.01,\n",
    "    column_titles=features_wrapped,\n",
    "    row_titles=features_wrapped,\n",
    ")\n",
    "\n",
    "for i in range(num_features):  # iterating over rows\n",
    "    for j in range(num_features):  # iterating over columns\n",
    "        if i == j:  # do not plot anything on the diagonal\n",
    "            x = efficacy_df[all_features[i]]\n",
    "            nx = 10\n",
    "            xs = np.linspace(x.min(), x.max(), nx + 1)\n",
    "            y = []\n",
    "            for k in range(nx):\n",
    "                eff_k = efficacy_df[\n",
    "                    (efficacy_df[all_features[i]] >= xs[k])\n",
    "                    & (efficacy_df[all_features[i]] < xs[k + 1])\n",
    "                ][\"score_proxy\"]\n",
    "                y.append(eff_k.mean())\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=xs[:-1] + 0.5 * (xs[1] - xs[0]), y=y, mode=\"lines\"),\n",
    "                row=i + 1,\n",
    "                col=j + 1,\n",
    "            )\n",
    "        else:\n",
    "            fig.add_trace(\n",
    "                go.Scattergl(\n",
    "                    x=efficacy_df[all_features[j]],\n",
    "                    y=efficacy_df[all_features[i]],\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(\n",
    "                        size=5,\n",
    "                        color=efficacy_df[\"score_proxy\"],\n",
    "                        opacity=efficacy_df[\"score_proxy\"] / max_score,\n",
    "                        coloraxis=\"coloraxis1\",\n",
    "                    ),\n",
    "                    hoverinfo=\"none\",\n",
    "                ),\n",
    "                row=i + 1,\n",
    "                col=j + 1,\n",
    "            )\n",
    "fig.update_annotations(font_size=12)\n",
    "fig.update_coloraxes(\n",
    "    colorbar_title=\"score proxy\",\n",
    "    cmin=min_score,\n",
    "    cmax=max_score,\n",
    "    colorscale=\"rainbow\",\n",
    "    colorbar_thickness=15,\n",
    "    colorbar_title_side=\"right\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    font=dict(size=14),\n",
    "    showlegend=False,\n",
    "    width=1000,\n",
    "    height=900,\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = min(5, len(all_features))\n",
    "features = all_features[:num_features]\n",
    "print(f\"Selected features: {features}\")\n",
    "\n",
    "\n",
    "def to_row(x_vec, columns):\n",
    "    return {columns[i]: x for i, x in enumerate(x_vec)}\n",
    "\n",
    "\n",
    "def lower_bound(d):\n",
    "    return f\"{d}_lower_bound\"\n",
    "\n",
    "\n",
    "def width(d):\n",
    "    return f\"{d}_width\"\n",
    "\n",
    "\n",
    "feature_bounds = {\n",
    "    d: (cross_arm_scalars.min(axis=0)[d], cross_arm_scalars.max(axis=0)[d])\n",
    "    for d in features\n",
    "}\n",
    "column_bounds = {}\n",
    "for d in features:\n",
    "    (d_min, d_max) = feature_bounds[d]\n",
    "    column_bounds[lower_bound(d)] = (d_min, d_max - 0.1 * (d_max - d_min))\n",
    "    column_bounds[width(d)] = (0.1 * (d_max - d_min), d_max - d_min)\n",
    "\n",
    "dim = len(column_bounds)\n",
    "column_keys = list(column_bounds.keys())\n",
    "\n",
    "\n",
    "def format_eligibility_criteria(criteria):\n",
    "    s = \"\"\n",
    "    for f in features:\n",
    "        s += f\"\\n {criteria[lower_bound(f)]:.3g} <= {f} <= {criteria[lower_bound(f)] + criteria[width(f)]:.3g}\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficacy tooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_conditions(row):\n",
    "    conditions = [\n",
    "        (cross_arm_scalars[d] >= row[lower_bound(d)])\n",
    "        & (cross_arm_scalars[d] <= row[lower_bound(d)] + row[width(d)])\n",
    "        for d in features\n",
    "    ]\n",
    "    return np.logical_and.reduce(conditions)\n",
    "\n",
    "\n",
    "def group_size(row):\n",
    "    return len(cross_arm_scalars[filter_conditions(row)])\n",
    "\n",
    "\n",
    "def filter(row):\n",
    "    if group_size(row) < sample_size * min_resp_vp_size:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def score(row, seed=cookbook_seed, n_boot=num_bootstraps, verbose=False):\n",
    "    mean_efficacy, sd_efficacy = gross_efficacy(row, seed=seed, n_boot=n_boot)\n",
    "    if verbose:\n",
    "        print(f\"mean, std of gross efficacy = {mean_efficacy:.3g}, {sd_efficacy:.2g}\")\n",
    "    if mean_efficacy and sd_efficacy:\n",
    "        return objective_function(\n",
    "            efficacy=mean_efficacy,\n",
    "            efficacy_sd=sd_efficacy,\n",
    "        )\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def gross_efficacy(row, seed=cookbook_seed, n_boot=num_bootstraps):\n",
    "    # A reproducible random generator whose seed depends on the top-level cookbook seed AND the evaluated design row\n",
    "    seeds = [seed, abs(hash(frozenset(row.items())))]\n",
    "    # Creating the corresponding control and treated filtered dataset\n",
    "    control_filtered = control_arm_scalars[filter_conditions(row)][outcome_name]\n",
    "    treated_filtered = treated_arm_scalars[filter_conditions(row)][outcome_name]\n",
    "    if len(control_filtered) < sample_size * min_resp_vp_size:\n",
    "        return None, None\n",
    "    else:\n",
    "        return bootstrapped_gross_efficacy(control_filtered, treated_filtered, seeds, n_boot=n_boot)\n",
    "\n",
    "def bootstrapped_gross_efficacy(control_filtered, treated_filtered, seeds, n_boot):\n",
    "    # A reproducible random generator whose seed depends on the top-level cookbook seed AND the evaluated design row\n",
    "    rng = np.random.default_rng(seeds)\n",
    "    bootstrap_gross_efficacies = []\n",
    "    all_indices = np.arange(len(control_filtered))\n",
    "    for _ in range(n_boot):\n",
    "        # pick 2 * sample_size indices at random from the full dataset\n",
    "        shuffled_indices = rng.choice(all_indices, size=2 * sample_size, replace=False)\n",
    "        # first sample_size indices for the control sub-group\n",
    "        ctrl_mean = control_filtered.iloc[shuffled_indices[:sample_size]].mean()\n",
    "        # next sample_size indices for the treated sub-group\n",
    "        trtd_mean = treated_filtered.iloc[\n",
    "            shuffled_indices[sample_size : 2 * sample_size]\n",
    "        ].mean()\n",
    "        bootstrap_gross_efficacies.append(ctrl_mean - trtd_mean)\n",
    "\n",
    "    return np.mean(bootstrap_gross_efficacies), np.std(\n",
    "        bootstrap_gross_efficacies, ddof=1\n",
    "    )\n",
    "\n",
    "def score_proxy(row):\n",
    "    control_filtered = control_arm_scalars[filter_conditions(row)][outcome_name]\n",
    "    treated_filtered = treated_arm_scalars[filter_conditions(row)][outcome_name]\n",
    "\n",
    "    if len(control_filtered) < sample_size * min_resp_vp_size:\n",
    "        return None\n",
    "\n",
    "    mean_efficacy = control_filtered.mean() - treated_filtered.mean()\n",
    "    var_efficacy = (control_filtered.var() + treated_filtered.var()) / sample_size\n",
    "\n",
    "    return objective_function(\n",
    "        efficacy=mean_efficacy,\n",
    "        efficacy_sd=np.sqrt(var_efficacy),\n",
    "    )\n",
    "\n",
    "\n",
    "def net_efficacy(row):\n",
    "    control_filtered = control_arm_scalars[filter_conditions(row)]\n",
    "    treated_filtered = treated_arm_scalars[filter_conditions(row)]\n",
    "    efficacy_df = pd.merge(\n",
    "        control_filtered, treated_filtered[outcome_name], how=\"inner\", on=\"patientId\"\n",
    "    )\n",
    "    efficacy_df[\"net_efficacy\"] = (\n",
    "        efficacy_df[f\"{outcome_name}_x\"] - efficacy_df[f\"{outcome_name}_y\"]\n",
    "    )\n",
    "    mean_efficacy, std_efficacy = (\n",
    "        efficacy_df[\"net_efficacy\"].mean(),\n",
    "        efficacy_df[\"net_efficacy\"].std(),\n",
    "    )\n",
    "    return (mean_efficacy, std_efficacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eligibility criteria optimization using score proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imax = efficacy_df[\"score_proxy\"].argmax()\n",
    "print(\n",
    "    f\"Best score proxy = {efficacy_df.iloc[imax][\"score_proxy\"]:.3g}, index of patient with best score proxy: {imax}\"\n",
    ")\n",
    "display(efficacy_df.iloc[imax][features])\n",
    "x0_unscaled = efficacy_df.iloc[imax][features].to_numpy()\n",
    "\n",
    "X_unscaled = efficacy_df[features].to_numpy()\n",
    "feature_scaler = StandardScaler().fit(X_unscaled)\n",
    "X = feature_scaler.transform(X_unscaled)\n",
    "neigh_regressor.fit(X, efficacy_df[f\"{outcome_name}_x\"].to_numpy())\n",
    "\n",
    "x0_scaled = feature_scaler.transform(np.reshape(x0_unscaled, (1, x0_unscaled.size)))\n",
    "distances, indices = neigh_regressor.kneighbors(\n",
    "    x0_scaled, n_neighbors=sample_size * min_resp_vp_size\n",
    ")\n",
    "\n",
    "neighbors_features_scaled = X[indices[0, :], :]\n",
    "neighbors_features_unscaled = feature_scaler.inverse_transform(\n",
    "    neighbors_features_scaled\n",
    ")\n",
    "score_proxy_criteria = {}\n",
    "for i, f in enumerate(features):\n",
    "    min_f, max_f = np.amin(neighbors_features_unscaled[:, i]), np.amax(\n",
    "        neighbors_features_unscaled[:, i]\n",
    "    )\n",
    "    score_proxy_criteria[lower_bound(f)] = min_f\n",
    "    score_proxy_criteria[width(f)] = max_f - min_f\n",
    "print(format_eligibility_criteria(score_proxy_criteria))\n",
    "\n",
    "n_boot = 100\n",
    "mean_ge, sd_ge = gross_efficacy(score_proxy_criteria, n_boot=n_boot)\n",
    "actual_score = objective_function(mean_ge, sd_ge)\n",
    "print(f\"\\nMean of gross efficacy (with {n_boot} bootstraps) = {mean_ge:.3g}\")\n",
    "print(f\"Standard deviation of gross efficacy (with {n_boot} bootstraps) = {sd_ge:.2g}\")\n",
    "print(f\"Actual score = {actual_score:.3g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eligibility criteria optimization using Sobol sequence grid search + MC fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobol sequence grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = st.qmc.Sobol(dim, scramble=False)\n",
    "m = 14\n",
    "print(f\"Full design size = {2**m}\")\n",
    "scaled_samples = sampler.random_base2(m)  # generates 2**m points\n",
    "\n",
    "\n",
    "def unscale(x):\n",
    "    return st.qmc.scale(\n",
    "        x,\n",
    "        [column_bounds[c][0] for c in column_keys],\n",
    "        [column_bounds[c][1] for c in column_keys],\n",
    "    )\n",
    "\n",
    "\n",
    "def scale(x):\n",
    "    return st.qmc.scale(\n",
    "        x,\n",
    "        [column_bounds[c][0] for c in column_keys],\n",
    "        [column_bounds[c][1] for c in column_keys],\n",
    "        reverse=True,\n",
    "    )\n",
    "\n",
    "\n",
    "samples = unscale(scaled_samples)\n",
    "\n",
    "filtered_indices = []\n",
    "for i, x in enumerate(samples):\n",
    "    row = to_row(x, columns=column_keys)\n",
    "    if filter(row):\n",
    "        filtered_indices.append(i)\n",
    "print(f\"Number of admissible points = {len(filtered_indices)} / {samples.shape[0]}\")\n",
    "filtered_samples = samples[filtered_indices, :]\n",
    "\n",
    "y = [score_proxy(to_row(x, columns=column_keys)) for x in filtered_samples]\n",
    "design_i_max, design_y_max = np.argmax(y), np.amax(y)\n",
    "exhaustive_best_criteria = to_row(filtered_samples[design_i_max], columns=column_keys)\n",
    "print(\n",
    "    \"Best eligibility criteria:\\n\"\n",
    "    + format_eligibility_criteria(exhaustive_best_criteria)\n",
    ")\n",
    "print(f\"\\nBest score proxy = {design_y_max:.3g}\")\n",
    "\n",
    "n_boot = 100\n",
    "mean_ge, sd_ge = gross_efficacy(exhaustive_best_criteria, n_boot=n_boot)\n",
    "actual_score = objective_function(mean_ge, sd_ge)\n",
    "print(f\"Mean of gross efficacy (with {n_boot} bootstraps) = {mean_ge:.3g}\")\n",
    "print(f\"Standard deviation of gross efficacy (with {n_boot} bootstraps) = {sd_ge:.2g}\")\n",
    "print(f\"Actual score = {actual_score:.3g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having found the best point in the Sobol design of experiment, let's search for an even better set of\n",
    "eligibility criteria around said point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_x = scale(filtered_samples)\n",
    "tree = KDTree(scaled_x)\n",
    "k = 10\n",
    "x0 = scaled_x[design_i_max, :]\n",
    "distances, indices = tree.query(np.reshape(x0, (1, x0.size)), k=k)\n",
    "# distance to kth nearest neighbor\n",
    "radius = distances[0, k - 1]\n",
    "\n",
    "rng = np.random.default_rng([cookbook_seed])\n",
    "num_mc = 10000\n",
    "# the average squared distance from the mean in a multivariate normal distribution is tr(Î£)\n",
    "# so by choosing a standard deviation of sigma = radius / sqrt(dim), our sampled points will lie in average\n",
    "# at a distance of radius from x0\n",
    "scaled_mc = rng.normal(loc=x0, scale=radius / np.sqrt(dim), size=(num_mc, dim))\n",
    "scaled_mc[scaled_mc < 0] = 0\n",
    "scaled_mc[scaled_mc > 1] = 1\n",
    "X_mc = unscale(scaled_mc)\n",
    "filtered_indices = []\n",
    "for i, x in enumerate(X_mc):\n",
    "    row = to_row(x, columns=column_keys)\n",
    "    if filter(row):\n",
    "        filtered_indices.append(i)\n",
    "\n",
    "print(\n",
    "    f\"MC fine-tuning, number of admissible points = {len(filtered_indices)} / {X_mc.shape[0]} \"\n",
    ")\n",
    "X_mc_filtered = X_mc[filtered_indices, :]\n",
    "y_mc = np.array([score_proxy(to_row(x, columns=column_keys)) for x in X_mc_filtered])\n",
    "mc_i_max = np.argmax(y_mc)\n",
    "mc_best_score = y_mc[mc_i_max]\n",
    "mc_best_criteria = to_row(X_mc_filtered[mc_i_max], columns=column_keys)\n",
    "print(\"Best eligibility criteria:\\n\" + format_eligibility_criteria(mc_best_criteria))\n",
    "print(f\"\\nBest score proxy = {mc_best_score:.3g}\")\n",
    "\n",
    "n_boot = 100\n",
    "mean_ge, sd_ge = gross_efficacy(mc_best_criteria, n_boot=n_boot)\n",
    "actual_score = objective_function(mean_ge, sd_ge)\n",
    "print(f\"Mean of gross efficacy (with {n_boot} bootstraps) = {mean_ge:.3g}\")\n",
    "print(f\"Standard deviation of gross efficacy (with {n_boot} bootstraps) = {sd_ge:.2g}\")\n",
    "print(f\"Actual score = {actual_score:.3g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(criteria, n_boot):\n",
    "    num_responding = group_size(criteria)\n",
    "    print(f\"  Number of unique best responding patients: {num_responding}\")\n",
    "    mean_gross_efficacy, sd_gross_efficacy = gross_efficacy(criteria, n_boot=n_boot)\n",
    "    print(\n",
    "        f\"  Mean of gross efficacy (with {n_boot} bootstraps) = {mean_gross_efficacy:.3g}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Standard deviation of gross efficacy (with {n_boot} bootstraps) = {sd_gross_efficacy:.2g}\"\n",
    "    )\n",
    "    control_filtered = control_arm_scalars[filter_conditions(criteria)][outcome_name]\n",
    "    treated_filtered = treated_arm_scalars[filter_conditions(criteria)][outcome_name]\n",
    "    best_responders_net_efficacy = control_filtered.mean() - treated_filtered.mean()\n",
    "    print(\n",
    "        f\"  Net efficacy in the population of best responders = {best_responders_net_efficacy:.3g}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"    --> gain in net efficacy = {best_responders_net_efficacy - initial_net_efficacy:.3g} (+{(best_responders_net_efficacy - initial_net_efficacy)/initial_net_efficacy:.0%})\"\n",
    "    )\n",
    "    control_outcome_std = control_filtered.std()\n",
    "    best_sample_size = math.ceil(\n",
    "        sample_size_continuous_outcome(\n",
    "            alpha, beta, best_responders_net_efficacy, control_outcome_std\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        f\"  Required sample size in population of best responders = {best_sample_size}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"    --> gain in required sample size = {sample_size - best_sample_size} (-{(sample_size - best_sample_size)/sample_size:.0%})\"\n",
    "    )\n",
    "\n",
    "\n",
    "n_boot = 50\n",
    "print(f\"Net efficacy in initial population = {initial_net_efficacy:.3g}\")\n",
    "mean_gross_efficacy, sd_gross_efficacy = bootstrapped_gross_efficacy(control_arm_scalars[outcome_name],treated_arm_scalars[outcome_name], seeds = [0], n_boot=n_boot)\n",
    "print(f\"Mean of gross efficacy (with {n_boot} bootstraps) = {mean_gross_efficacy:.3g}\")\n",
    "print(\n",
    "    f\"Standard deviation of gross efficacy (with {n_boot} bootstraps) = {sd_gross_efficacy:.2g}\"\n",
    ")\n",
    "print(f\"Required sample size in initial population = {sample_size}\")\n",
    "\n",
    "\n",
    "print(\"\\nEligibility criteria using Sobol design exhaustive search + MC fine-tuning:\")\n",
    "print_report(mc_best_criteria, n_boot=n_boot)\n",
    "\n",
    "print(\"\\nEligibility criteria using score proxy:\")\n",
    "print_report(score_proxy_criteria, n_boot=n_boot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jinko-api-cookbook-0CE9nVuV-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
